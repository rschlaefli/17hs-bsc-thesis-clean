{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook examines the functionality of our dataset standardization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\envs\\bsc-thesis\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import paths\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from TRMM import TRMM\n",
    "from ERA import ERA\n",
    "from ModelHelpers import ModelHelpers\n",
    "\n",
    "def train_test_split(datasets,\n",
    "                         prediction_ts,\n",
    "                         prediction_ts_test,\n",
    "                         onset_ts,\n",
    "                         years=range(1979, 2018),\n",
    "                         years_train=range(1979, 2010),\n",
    "                         years_dev=range(2010, 2013),\n",
    "                         years_test=range(2013, 2018)):\n",
    "        \"\"\"\n",
    "        Prepare data to be in a digestible format for the model\n",
    "\n",
    "        :datasets: List of datasets to use as features\n",
    "        :outcomes: Outcomes as generated by the base model\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # generate outcomes\n",
    "        outcomes = ModelHelpers.generate_outcomes(prediction_ts, onset_ts, chain(years_train, years_dev), numerical=True, sequence=True)\n",
    "        outcomes_test = ModelHelpers.generate_outcomes(prediction_ts_test, onset_ts, years_test, numerical=True, sequence=True)\n",
    "        print(outcomes_test)\n",
    "\n",
    "        # generate training data\n",
    "        X_train = ModelHelpers.prepare_datasets(years_train, datasets, prediction_ts)\n",
    "        y_train = ModelHelpers.stack_outcomes(outcomes, years_train, augmented=True)\n",
    "        print(X_train[0][0][0])\n",
    "        print('> X_train', X_train.shape, 'y_train', y_train.shape)\n",
    "\n",
    "        X_train, X_mean, X_std = ModelHelpers.normalize_channels(X_train, seperate=True)\n",
    "        print(X_mean)\n",
    "        print(X_std)\n",
    "\n",
    "        # generate test data\n",
    "        X_test = ModelHelpers.prepare_datasets(years_test, datasets, prediction_ts_test)\n",
    "        y_test = ModelHelpers.stack_outcomes(outcomes_test, years_test, augmented=True)\n",
    "        print(X_test.shape)\n",
    "        print('> X_test', X_test.shape, 'y_test', y_test.shape)\n",
    "\n",
    "        X_test = ModelHelpers.normalize_channels(X_test, mean=X_mean, std=X_std)\n",
    "        \n",
    "def normalize_channels(arr, standardize=True, seperate=False, mean=None, std=None):\n",
    "        \"\"\" Normalize or standardize channels of a 4D tensor \"\"\"\n",
    "\n",
    "        # normalize each channel seperately\n",
    "        # axes 1 and 2 should be fixed (lat and lon)\n",
    "        # axes 0 and 2 should be variable (for each channel seperately over all images)\n",
    "        # see: https://stackoverflow.com/questions/42460217/how-to-normalize-a-4d-numpy-array\n",
    "        # and: https://stackoverflow.com/questions/40956114/numpy-standardize-2d-subsets-of-a-4d-array\n",
    "\n",
    "        # the channels should be standardized to zero mean and unit variance seperately\n",
    "        if mean is not None and std is not None:\n",
    "            return (arr - mean) / std\n",
    "\n",
    "\n",
    "        mean = np.mean(arr, axis=(0, 1), keepdims=True)\n",
    "        std = np.std(arr, axis=(0, 1), keepdims=True)\n",
    "\n",
    "        return (arr - mean) / std, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading Dataset\n",
      "> Loading from cache...\n",
      "> Loading from cache...\n",
      "Processed 2010 (30, 61, 49, 49, 2)\n",
      "Processed 2011 (30, 61, 49, 49, 2)\n",
      "Processed 2012 (30, 61, 49, 49, 2)\n",
      "Processed 2013 (30, 61, 49, 49, 2)\n",
      "Processed 2014 (30, 61, 49, 49, 2)\n",
      "Processed 2015 (30, 61, 49, 49, 2)\n",
      "Processed 2016 (30, 61, 49, 49, 2)\n",
      "Processed 2017 (30, 61, 49, 49, 2)\n"
     ]
    }
   ],
   "source": [
    "YEARS = range(2010, 2018)\n",
    "YEARS_TRAIN = range(2010, 2014)\n",
    "YEARS_TEST = range(2014, 2018)\n",
    "\n",
    "# load onset dates\n",
    "onset_dates, onset_ts = ModelHelpers.load_onset_dates(version='v2', objective=True)\n",
    "\n",
    "# prepare prediction timestamps\n",
    "# generate a sequence of timestamps for train and validation (and, optionally, test)\n",
    "prediction_ts = ModelHelpers.generate_prediction_ts('{}-05-22', YEARS_TRAIN, onset_dates=onset_dates, sequence_length=29, sequence_offset=1, example_length=60)\n",
    "prediction_ts_test = ModelHelpers.generate_prediction_ts('{}-05-22', YEARS_TEST, onset_dates=onset_dates, sequence_length=29, sequence_offset=1, example_length=60)\n",
    "\n",
    "# setup a filter function\n",
    "# this later prevents any data after the prediction timestamp from being fed as input\n",
    "# we do this externally to allow overriding or extending the filter function if needed\n",
    "def filter_fun(df, year):\n",
    "    return ModelHelpers.filter_until(df, onset_ts[year])\n",
    "\n",
    "# load the ERA dataset\n",
    "print(\"> Loading Dataset\")\n",
    "dataset = ERA.load_dataset_v2(range(2010, 2018), invalidate=False, level=1000, variables=['r', 't'], filter_fun=filter_fun, aggregation_resolution=None)\n",
    "\n",
    "# generate training data\n",
    "X_train = ModelHelpers.prepare_datasets(YEARS_TRAIN, [dataset['r'],dataset['t']], prediction_ts)\n",
    "X_test = ModelHelpers.prepare_datasets(YEARS_TEST, [dataset['r'],dataset['t']], prediction_ts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_mean, X_std = normalize_channels(X_train, seperate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 61, 49, 49, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 61, 49, 49, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 49, 49, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 49, 49, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[ 76.03646987, 300.25469346],\n",
       "          [ 76.00147785, 300.27541551],\n",
       "          [ 75.75109012, 300.28128868],\n",
       "          ...,\n",
       "          [ 83.39855597, 299.93071621],\n",
       "          [ 88.18274285, 300.52245945],\n",
       "          [ 88.25598933, 300.92124126]],\n",
       "\n",
       "         [[ 75.45281735, 300.3042589 ],\n",
       "          [ 75.34161266, 300.29842613],\n",
       "          [ 75.42493168, 300.30611566],\n",
       "          ...,\n",
       "          [ 81.83860695, 300.19316423],\n",
       "          [ 80.94718736, 300.49322597],\n",
       "          [ 78.32655993, 300.74613697]],\n",
       "\n",
       "         [[ 75.61353495, 300.27163686],\n",
       "          [ 75.62301163, 300.24182219],\n",
       "          [ 75.29544047, 300.31048001],\n",
       "          ...,\n",
       "          [ 76.23657294, 300.78393032],\n",
       "          [ 76.09455551, 300.88782531],\n",
       "          [ 76.23910005, 300.96528742]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 39.98055257, 293.501531  ],\n",
       "          [ 40.99420661, 293.75472701],\n",
       "          [ 42.37022626, 294.0627037 ],\n",
       "          ...,\n",
       "          [ 39.69289786, 293.26210441],\n",
       "          [ 43.75700562, 293.19795277],\n",
       "          [ 43.7785655 , 293.16502892]],\n",
       "\n",
       "         [[ 41.81813648, 292.90789972],\n",
       "          [ 41.30280944, 293.25660076],\n",
       "          [ 41.20353854, 293.57835744],\n",
       "          ...,\n",
       "          [ 28.95101502, 293.64823004],\n",
       "          [ 31.3149454 , 293.47573463],\n",
       "          [ 30.17361114, 293.37441319]],\n",
       "\n",
       "         [[ 40.81931286, 292.16544405],\n",
       "          [ 40.97849551, 292.4744589 ],\n",
       "          [ 40.72362659, 292.92429077],\n",
       "          ...,\n",
       "          [ 20.92024511, 294.81087794],\n",
       "          [ 21.92241255, 294.4328187 ],\n",
       "          [ 20.69778456, 294.21976877]]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08179388181343744"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((X_test - X_mean) / X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
