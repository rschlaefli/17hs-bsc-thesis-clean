import os
import pathlib
import time
import pickle
import re
import hashlib
import numpy as np
import pandas as pd

from sklearn import preprocessing
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from keras.models import Model, Sequential, load_model
from keras.layers import BatchNormalization, Dropout, LSTM, Dense, Conv2D, ConvLSTM2D, Flatten, MaxPooling2D, MaxPooling3D
from keras.metrics import categorical_accuracy, top_k_categorical_accuracy
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import Adam, RMSprop, SGD
from keras import backend as K
from keras.backend.tensorflow_backend import set_session
from keras.callbacks import Callback, EarlyStopping, TensorBoard, EarlyStopping, CSVLogger
from keras.utils import to_categorical

from ModelHelpers import ModelHelpers

# limit gpu resource usage of tensorflow
# see https://github.com/keras-team/keras/issues/1538
import tensorflow as tf
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

class ModelTRMMv3:
    """ Version 3 of LSTM model """

    def __init__(self):
        self.cache_path = None
        self.model = None
        self.history = None
        self.results = None
        self.log_path = None
        self.log_name = None

    @staticmethod
    def train_test_split(trmm_data,
                         prediction_ts,
                         onset_ts,
                         numerical=False,
                         years=range(1998, 2017),
                         years_train=range(1998, 2016),
                         years_dev=None,
                         years_test=range(2016, 2017)):
        """
        Prepare data to be in a digestible format for the model

        :trmm_data: Filtered and optionally aggregated TRMM dataset to use
        :outcomes: Outcomes as generated by the base model

        :return:
        """

        def unstack_year(df):
            """ Unstack a single year and return an unstacked sequence of grids """

            return np.array(
                [df.iloc[:, i].unstack().values for i in range(df.shape[1])])

        def unstack_all(df):
            """ Unstack all years and return the resulting dict """

            result = {}

            for year in years:
                result[year] = unstack_year(df[year])

            return result

        def reshape_years(arr):
            return np.array(
                list(
                    map(lambda year: year.reshape((year.shape[0], year.shape[1], year.shape[2], 1)),
                        arr)))

        def stack_outcomes(outcomes, years):
            if numerical:
                return [outcomes[year] for year in years]

            return np.concatenate([outcomes[year] for year in years])

        # generate outcomes
        outcomes = ModelHelpers.generate_outcomes(
            prediction_ts, onset_ts, years, numerical=numerical)

        # unstack the entire trmm dataset
        # => bring into matrix form with lat/lon on axes
        unstacked = unstack_all(trmm_data)

        # generate training data
        X_train = reshape_years([unstacked[year] for year in years_train])
        y_train = stack_outcomes(outcomes, years_train)

        # generate test data
        X_test = reshape_years([unstacked[year] for year in years_test])
        y_test = stack_outcomes(outcomes, years_test)

        if years_dev:
            X_dev = reshape_years([unstacked[year] for year in years_dev])
            y_dev = stack_outcomes(outcomes, years_dev)

            return X_train, y_train, X_test, y_test, X_dev, y_dev, unstacked

        return X_train, y_train, X_test, y_test, unstacked

    def build(self,
              X_train,
              y_train,
              dropout=0.6,
              dropout_recurrent=None,
              dropout_conv=0.4,
              epochs=50,
              optimizer='rmsprop',
              learning_rate=None,
              batch_size=1,
              numerical_loss='mean_squared_error',
              categorical_loss='categorical_crossentropy',
              batch_norm=True,
              num_filters=(32, 16, 8),
              kernel_dims=(7, 5, 3),
              padding='same',
              pool_dims=(0, 0, 0, 4),
              numerical=True,
              dense_nodes=1024,
              dense_activation='relu',
              recurrent_activation='relu',
              patience=0,
              validation_split=0.1,
              validation_data=None,
              evaluate=None,
              cache_id=None,
              version='T3',
              verbose=1,
              invalidate=False,
              tensorboard=False):
        """
        Build a stateless LSTM model

        :X_train: Training features
        :y_train: Training outcomes
        :dropout: How much dropout to use after dense layers
        :dropout_recurrent: How much recurrent dropout to use in ConvLSTM2D
        :dropout_conv: How much dropout to use after convolutional layers
        :epochs: How many epochs to train for
        :batch_size: The batch size of training
        :numerical_loss: The loss function to use
        :categorical_loss: The loss function to use
        :optimizer: The optimizer to use
        :learning_rate: The learning rate to use with the optimizer
        :batch_norm: Whether to use batch normalization
        :num_filters: The number of filters for the first, second and third conv layer as a tuple
        :kernel_dims: The dimensions of the kernels (squared, so 7 means 7x7)
        :padding: Whether to apply padding or not
        :pool_dims: Dimensions of the max pooling layers => one after each conv layer, final one before flatten
        :numerical: Whether to train classes or numerical output
        :dense_nodes: The number of dense nodes at the start of dense layers
        :dense_activation: The activation to use in dense nodes
        :recurrent_activation: The activation to use in the ConvLSTM2D layers
        :verbose:
        :invalidate:
        :patience:
        :tensorboard:
        :validation_split:


        :return: The fitted model
        :return: The history of training the model
        """

        current_path = pathlib.Path(__file__).resolve().parent.parent.parent

        # define a path for the model to be cached to
        # => use md5 or similar for cache path..
        formatted = 'ep-{}_bat-{}_opt-{}_lr-{}_dpt-{}_dptR-{}_dptC-{}_ls-{}_flt-{}_krn-{}_pool-{}_pad-{}_pat-{}_norm-{}_dnsNod-{}_dnsAct-{}_recAct-{}_splt-{}'.format(
            str(epochs), str(batch_size), str(optimizer), str(learning_rate),
            str(dropout), str(dropout_recurrent), str(dropout_conv),
            str(numerical_loss if numerical else categorical_loss),
            str(num_filters), str(kernel_dims), str(pool_dims), str(padding),
            str(patience), str(batch_norm), str(dense_nodes),
            str(dense_activation), str(recurrent_activation),
            str(validation_split))
        print(formatted)
        hashed = hashlib.md5(formatted.encode())
        self.cache_path = current_path / '00_CACHE'
        self.cache_name = f'/lstm_{version}_{cache_id}_{"numerical" if numerical else "categorical"}_{hashed.hexdigest()}'
        self.log_path = current_path.parent / '03_EVALUATION/histories'
        self.log_name = f'lstm_{version}_{cache_id}'

        # try to read the model and its history from cache
        if not invalidate and os.path.isfile(
                self.cache_path / f'{self.cache_name}.h5') and os.path.isfile(
                    self.cache_path / f'{self.cache_name}.history'):
            model = load_model(self.cache_path / f'{self.cache_name}.h5')

            history = None
            with open(self.cache_path / f'{self.cache_name}.history',
                      'rb') as file:
                history = pickle.load(file)

            self.model = model
            self.history = history
            return

        input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3],
                       X_train.shape[4])

        # start building a sequential model
        model = Sequential()

        # add a first convolutional LSTM layer
        model.add(
            ConvLSTM2D(
                filters=num_filters[0],
                kernel_size=(kernel_dims[0], kernel_dims[0]),
                activation=recurrent_activation,
                padding=padding,
                input_shape=input_shape,
                return_sequences=True))
        if pool_dims[0] > 0:
            model.add(MaxPooling3D(pool_size=(1, pool_dims[0], pool_dims[0])))
        if batch_norm:
            model.add(BatchNormalization())
        if dropout_conv:
            model.add(Dropout(dropout_conv))

        # add a second convolutional LSTM layer
        model.add(
            ConvLSTM2D(
                filters=num_filters[1],
                kernel_size=(kernel_dims[1], kernel_dims[1]),
                activation=recurrent_activation,
                padding=padding,
                return_sequences=True))
        if pool_dims[1] > 0:
            model.add(MaxPooling3D(pool_size=(1, pool_dims[1], pool_dims[1])))
        if batch_norm:
            model.add(BatchNormalization())
        if dropout_conv:
            model.add(Dropout(dropout_conv))

        # add a third convolutional LSTM layer
        model.add(
            ConvLSTM2D(
                filters=num_filters[2],
                kernel_size=(kernel_dims[2], kernel_dims[2]),
                activation=recurrent_activation,
                padding=padding,
                return_sequences=False))
        if pool_dims[2] > 0:
            model.add(MaxPooling2D(pool_size=(pool_dims[2], pool_dims[2])))
        if batch_norm:
            model.add(BatchNormalization())
        if dropout_conv:
            model.add(Dropout(dropout_conv))

        # add max pooling before flattening to reduce the dimensionality
        if pool_dims[3] > 0:
            model.add(
                MaxPooling2D(
                    pool_size=(pool_dims[3], pool_dims[3]), padding=padding))

        # flatten to make data digestible for dense layers
        model.add(Flatten())
        if batch_norm:
            model.add(BatchNormalization())

        # first dense layer
        model.add(Dense(dense_nodes, activation=dense_activation))
        if batch_norm:
            model.add(BatchNormalization())
        if dropout:
            model.add(Dropout(dropout))

        # second dense layer
        model.add(Dense(round(dense_nodes / 2), activation=dense_activation))
        if batch_norm:
            model.add(BatchNormalization())
        if dropout:
            model.add(Dropout(dropout))

        if optimizer == 'rmsprop':
            if learning_rate:
                optimizer = RMSprop(lr=learning_rate)
            else:
                optimizer = RMSprop()
        elif optimizer == 'adam':
            if learning_rate:
                optimizer = Adam(lr=learning_rate)
            else:
                optimizer = Adam()
        elif optimizer == 'sgd':
            if learning_rate:
                optimizer = SGD(lr=learning_rate)
            else:
                optimizer = SGD()

        if numerical:
            # third dense layer
            model.add(
                Dense(round(dense_nodes / 4), activation=dense_activation))
            if batch_norm:
                model.add(BatchNormalization())
            if dropout:
                model.add(Dropout(dropout))

            # softmax layer for classification
            model.add(Dense(1))

            # compile the model
            model.compile(
                loss=numerical_loss,
                optimizer=optimizer,
                metrics=['mean_squared_error', 'mean_absolute_error'])
        else:
            # softmax layer for classification
            model.add(Dense(40, activation='softmax'))

            # compile the model
            model.compile(
                loss=categorical_loss,
                optimizer=optimizer,
                metrics=[categorical_accuracy, top_k_categorical_accuracy])

        # print an overview about the model
        print(model.summary())
        print('\n')

        # if early stopping patience is set, configure the callback
        callbacks = []
        if patience > 0:
            callbacks.append(
                EarlyStopping(monitor='val_loss', patience=patience))
        if tensorboard:
            callbacks.append(
                TensorBoard(log_dir=current_path / (
                    f'00_LOGS/{time.time()!s}')))

        log_path = str(self.log_path / f'{self.log_name}.csv')
        callbacks.append(CSVLogger(log_path))

        # fit the model to the training data
        # use 10% of the years as validation data
        history = model.fit(
            X_train,
            y_train,
            epochs=epochs,
            validation_split=validation_split,
            batch_size=batch_size,
            verbose=verbose,
            callbacks=callbacks)

        # save the model and its history to cache
        model.save(self.cache_path / f'{self.cache_name}.h5')
        with open(self.cache_path / f'{self.cache_name}.history',
                  'wb') as file:
            pickle.dump(history.history, file)

        self.model = model
        self.history = history.history
        """
        WIP: custom loss function

        def custom_loss(y_true, y_pred):
            print(y_true)
            print(y_pred)
            # TODO: buggy...
            print(K.int_shape(K.reshape(y_true, (1, 40))))
            print(K.int_shape(K.reshape(y_pred, (1, 40))))
            print(K.shape(K.abs(K.argmax(K.reshape(y_true, (1, 40))) -
                                K.argmax(K.reshape(y_pred, (1, 40))))))
            # return K.abs(K.argmax(K.reshape(y_true, (1, 40))) - K.argmax(K.reshape(y_pred, (1, 40))))
            return K.abs(K.argmax(K.reshape(y_true, (1, 40))) - K.argmax(K.reshape(y_pred, (1, 40))))

        model.compile(loss=custom_loss, optimizer=RMSprop(
            lr=0.001), metrics=[top_k_categorical_accuracy])
        """
        """

        WIP: batch learning (fitting the model in batches)
        for min_year, max_year in [(1998, 2002), (2002, 2006), (2006, 2010), (2010, 2014)]:
        year_range = range(min_year, max_year)

        X_train = reshape_years([unstacked[year] for year in year_range])
        y_train = stack_outcomes(outcomes, year_range)

        model.fit(X_train, y_train, validation_split=0.1, epochs=100, verbose=1, batch_size=1)

        model.fit(X_train, y_train, validation_split=0.1,
                   epochs=100, verbose=1, batch_size=1)
        """


"""
ARCHIVE

model2_input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4])

# start building a sequential model
model2 = Sequential()

model2.add(ConvLSTM2D(filters=32, kernel_size=(7, 7), padding='same', input_shape=model2_input_shape, return_sequences=True, dropout=0.5, recurrent_dropout=0.3))
# model2.add(MaxPooling3D(pool_size=(1, 4, 4)))
model2.add(BatchNormalization())

model2.add(ConvLSTM2D(filters=16, kernel_size=(5, 5), return_sequences=True, dropout=0.5, recurrent_dropout=0.3))
# model2.add(MaxPooling3D(pool_size=(1, 3, 3)))
model2.add(BatchNormalization())

model2.add(ConvLSTM2D(filters=8, kernel_size=(3, 3), return_sequences=False, dropout=0.5, recurrent_dropout=0.3))
# model2.add(MaxPooling3D(pool_size=(1, 3, 3)))
model2.add(BatchNormalization())

model2.add(Flatten())

model2.add(Dense(512, activation='relu'))
model2.add(BatchNormalization())
model2.add(Dropout(0.5))

model2.add(Dense(512, activation='relu'))
model2.add(BatchNormalization())
model2.add(Dropout(0.5))

model2.add(Dense(256, activation='relu'))
model2.add(BatchNormalization())
model2.add(Dropout(0.5))

model2.add(Dense(40, activation='softmax'))

model2.summary()

model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[top_k_categorical_accuracy])
Epoch 73/100
17/17 [==============================] - 12s - loss: 2.7670 - top_k_categorical_accuracy: 0.2353 - val_loss: 16.1181 - val_top_k_categorical_accuracy: 1.0000
Epoch 74/100
17/17 [==============================] - 12s - loss: 2.8185 - top_k_categorical_accuracy: 0.2941 - val_loss: 16.1181 - val_top_k_categorical_accuracy: 1.0000
Epoch 75/100
17/17 [==============================] - 12s - loss: 2.8145 - top_k_categorical_accuracy: 0.2941 - val_loss: 16.1181 - val_top_k_categorical_accuracy: 1.0000
Epoch 76/100
17/17 [==============================] - 12s - loss: 2.8189 - top_k_categorical_accuracy: 0.1765 - val_loss: 16.1181 - val_top_k_categorical_accuracy: 1.0000
Epoch 77/100
17/17 [==============================] - 12s - loss: 2.7903 - top_k_categorical_accuracy: 0.1765 - val_loss: 16.1181 - val_top_k_categorical_accuracy: 1.0000
Epoch 78/100
17/17 [==============================] - 12s - loss: 2.7889 - top_k_categorical_accuracy: 0.4118 - val_loss: 16.1181 - val_top_k_categorical_accuracy: 1.0000
"""
