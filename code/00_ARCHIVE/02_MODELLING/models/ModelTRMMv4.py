import os
import pathlib
import time
import pickle
import re
import hashlib
import numpy as np
import pandas as pd

from sklearn import preprocessing
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from keras.models import Model, Sequential, load_model
from keras.layers import BatchNormalization, Dropout, LSTM, Dense, Conv2D, ConvLSTM2D, Flatten, MaxPooling2D, MaxPooling3D
from keras.metrics import categorical_accuracy, top_k_categorical_accuracy
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import Adam, RMSprop, SGD
from keras import backend as K
from keras.backend.tensorflow_backend import set_session
from keras.callbacks import Callback, EarlyStopping, TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger
from keras.utils import to_categorical
from keras.regularizers import l1, l2, l1_l2

from ModelHelpers import ModelHelpers

# limit gpu resource usage of tensorflow
# see https://github.com/keras-team/keras/issues/1538
import tensorflow as tf
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

class ModelTRMMv4:
    """ Version 4 of LSTM model """

    def __init__(self):
        self.cache_path = None
        self.log_path = None
        self.model = None
        self.history = None
        self.results = None

    @staticmethod
    def train_test_split(trmm_data,
                         prediction_ts,
                         onset_ts,
                         years=range(1998, 2017),
                         years_train=range(1998, 2016),
                         years_dev=None,
                         years_test=range(2016, 2017)):
        """
        Prepare data to be in a digestible format for the model

        :trmm_data: Filtered and optionally aggregated TRMM dataset to use
        :outcomes: Outcomes as generated by the base model

        :return:
        """

        def unstack_year(df):
            """ Unstack a single year and return an unstacked sequence of grids """

            return np.array(
                [df.iloc[:, i].unstack().values for i in range(df.shape[1])])

        def unstack_all(df):
            """ Unstack all years and return the resulting dict """

            result = {}

            for year in years:
                result[year] = unstack_year(df[year])

            return result

        def reshape_years(arr):
            return np.array(
                list(
                    map(lambda year: year.reshape((year.shape[0], year.shape[1], year.shape[2], 1)),
                        arr)))

        def stack_outcomes(outcomes, years):
            return [outcomes[year] for year in years]

        # generate outcomes
        outcomes = ModelHelpers.generate_outcomes(
            prediction_ts, onset_ts, years, numerical=True)

        # unstack the entire trmm dataset
        # => bring into matrix form with lat/lon on axes
        unstacked = unstack_all(trmm_data)

        # generate training data
        X_train = reshape_years([unstacked[year] for year in years_train])
        y_train = stack_outcomes(outcomes, years_train)

        # generate test data
        X_test = reshape_years([unstacked[year] for year in years_test])
        y_test = stack_outcomes(outcomes, years_test)

        if years_dev:
            X_dev = reshape_years([unstacked[year] for year in years_dev])
            y_dev = stack_outcomes(outcomes, years_dev)

            return X_train, y_train, X_test, y_test, X_dev, y_dev, unstacked

        return X_train, y_train, X_test, y_test, unstacked

    def build(self,
              X_train,
              y_train,
              dropout=0.6,
              dropout_recurrent=None,
              dropout_conv=0.4,
              epochs=50,
              optimizer='rmsprop',
              learning_rate=None,
              batch_size=1,
              loss='mean_squared_error',
              batch_norm=True,
              conv_activation='tanh',
              conv_filters=[32, 16, 8],
              conv_kernels=[7, 5, 3],
              conv_pooling=[0, 0, 0, 4],
              recurrent_activation='hard_sigmoid',
              padding='same',
              dense_nodes=[1024, 512, 256],
              dense_activation='relu',
              patience=0,
              validation_split=0.1,
              validation_data=None,
              verbose=1,
              invalidate=False,
              tensorboard=False,
              numerical=True,
              dense_kernel_regularizer=None,
              conv_kernel_regularizer=None,
              conv_recurrent_regularizer=None,
              lr_plateau=False,
              cache_id=None,
              evaluate=None,
              version="T4"):
        """
        Build a stateless LSTM model

        :X_train: Training features
        :y_train: Training outcomes
        :dropout: How much dropout to use after dense layers
        :dropout_recurrent: How much recurrent dropout to use in ConvLSTM2D
        :dropout_conv: How much dropout to use after convolutional layers
        :epochs: How many epochs to train for
        :optimizer: The optimizer to use
        :learning_rate: The learning rate to use with the optimizer
        :batch_size: The batch size of training
        :loss: The loss function to use
        :batch_norm: Whether to use batch normalization
        :conv_activation: The activation to use in the convolutional layers
        :conv_filters: The number of filters for all convolutional layers as a list
        :conv_kernels: The dimensions of the kernels for all convolutional layers as a list
        :conv_pooling: Dimensions of the max pooling layers => one after each conv layer, final one before flatten
        :recurrent_activation: The activation for the LSTM recurrent part of ConvLSTM2D
        :padding: Whether to apply padding or not
        :numerical: Whether to train classes or numerical output
        :dense_nodes: The number of dense nodes for all dense layers as a list
        :dense_activation: The activation to use in all dense nodes except the final one
        :verbose: The level of logging to be used
        :invalidate: Whether the cache should be invalidated
        :patience: How much patience to use for early stopping
        :tensorboard: Whether tensorboard should be used
        :validation_split: How much of the data to keep for validation
        :conv_kernel_regularizer: Regularizer function applied to the kernel weights matrix of ConvLSTM2D layers.
        :conv_recurrent_regularizer: Regularizer function applied to the recurrent_kernel weights matrix of ConvLSTM2D layers.
        :dense_kernel_regularizer: Regularizer function applied to the kernel weights matrix of Dense layers.
        :lr_plateau: Whether the learning rate should be dynamically decreased when learning stagnates

        :return: The fitted model
        :return: The history of training the model
        """

        current_path = pathlib.Path(__file__).resolve().parent.parent.parent

        # define a path for the model to be cached to
        if cache_id is not None:
            # use the given cache id as a reference (array index)
            self.cache_path = current_path / '00_CACHE'
            self.cache_name = f'lstm_{version}_{cache_id}'
            self.log_path = current_path / '03_EVALUATION/histories'
            self.log_name = f'lstm_{version}_{cache_id}'
        else:
            # use md5 or similar for cache path..
            if (not conv_kernel_regularizer and not conv_recurrent_regularizer
                    and not dense_kernel_regularizer):
                formatted = 'ep-{}_bat-{}_opt-{}_lr-{}_dpt-{}_dptR-{}_dptC-{}_ls-{}_flt-{}_krn-{}_pool-{}_pad-{}_pat-{}_norm-{}_dnsNod-{}_dnsAct-{}_recAct-{}cnvAct-{}_splt-{}'.format(
                    str(epochs), str(batch_size), str(optimizer),
                    str(learning_rate), str(dropout), str(dropout_recurrent),
                    str(dropout_conv), str(loss), str(conv_filters),
                    str(conv_kernels), str(conv_pooling), str(padding),
                    str(patience), str(batch_norm), str(dense_nodes),
                    str(dense_activation), str(recurrent_activation),
                    str(conv_activation), str(validation_split))
            else:
                formatted = 'ep-{}_bat-{}_opt-{}_lr-{}_dpt-{}_dptR-{}_dptC-{}_ls-{}_flt-{}_krn-{}_pool-{}_pad-{}_pat-{}_norm-{}_dnsNod-{}_dnsAct-{}_recAct-{}cnvAct-{}_splt-{}_dnsReg-{}_convReg-{}_convRecReg-{}'.format(
                    str(epochs), str(batch_size), str(optimizer),
                    str(learning_rate), str(dropout), str(dropout_recurrent),
                    str(dropout_conv), str(loss), str(conv_filters),
                    str(conv_kernels), str(conv_pooling), str(padding),
                    str(patience), str(batch_norm), str(dense_nodes),
                    str(dense_activation), str(recurrent_activation),
                    str(conv_activation), str(validation_split),
                    str(dense_kernel_regularizer),
                    str(conv_kernel_regularizer),
                    str(conv_recurrent_regularizer))

            print(formatted)
            hashed = hashlib.md5(formatted.encode())
            self.cache_path = current_path / '00_CACHE'
            self.cache_name = f'lstm_{version}_{hashed.hexdigest()}'

        # try to read the model version to evaluate from cache
        # could either be latest or best
        if evaluate is not None:
            model = load_model(self.cache_path /
                               (self.cache_name + f'_{evaluate}.h5'))
            self.model = model
            return

        # try to read the model and its history from cache
        if not invalidate and os.path.isfile(
                self.cache_path / f'{self.cache_name}.h5') and os.path.isfile(
                    self.cache_path / f'{self.cache_name}.history'):
            model = load_model(self.cache_path / f'{self.cache_name}.h5')

            history = None
            with open(self.cache_path / f'{self.cache_name}.history',
                      'rb') as file:
                history = pickle.load(file)

            self.model = model
            self.history = history
            return

        # calculate the wanted input shape
        input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3],
                       X_train.shape[4])

        # prepare regularizers for later use in the model
        regularizers = {'L2': l2, 'L1': l1, 'L1_L2': l1_l2}
        conv_regularize_params = dict()
        dense_regularize_params = dict()
        if (conv_kernel_regularizer is not None):
            reg = regularizers[conv_kernel_regularizer[0]]
            conv_regularize_params['kernel_regularizer'] = reg(
                conv_kernel_regularizer[1])
        if (conv_recurrent_regularizer is not None):
            reg = regularizers[conv_recurrent_regularizer[0]]
            conv_regularize_params['recurrent_regularizer'] = reg(
                conv_recurrent_regularizer[1])
        if (dense_kernel_regularizer is not None):
            reg = regularizers[dense_kernel_regularizer[0]]
            dense_regularize_params['kernel_regularizer'] = reg(
                dense_kernel_regularizer[1])

        # start building a sequential model
        model = Sequential()

        # go through all convolutional lstm layers
        for (index, filters) in enumerate(conv_filters):
            last_layer = index == len(conv_filters) - 1

            # add a ConvLSTM2D layer
            if index == 0:
                model.add(
                    ConvLSTM2D(
                        filters=filters,
                        kernel_size=(conv_kernels[index], conv_kernels[index]),
                        activation=conv_activation,
                        recurrent_activation=recurrent_activation,
                        padding=padding,
                        input_shape=input_shape,
                        return_sequences=not last_layer,
                        **conv_regularize_params))
            else:
                model.add(
                    ConvLSTM2D(
                        filters=filters,
                        kernel_size=(conv_kernels[index], conv_kernels[index]),
                        activation=conv_activation,
                        recurrent_activation=recurrent_activation,
                        padding=padding,
                        return_sequences=not last_layer,
                        **conv_regularize_params))

            # add a pooling layer if configured
            if conv_pooling[index] > 0:
                if last_layer:
                    model.add(
                        MaxPooling2D(
                            pool_size=(conv_pooling[index],
                                       conv_pooling[index])))
                else:
                    model.add(
                        MaxPooling3D(
                            pool_size=(1, conv_pooling[index],
                                       conv_pooling[index])))

            # add batch normalization
            if batch_norm:
                model.add(BatchNormalization())

            # add dropout
            if dropout:
                model.add(Dropout(dropout))

        # add max pooling before flattening to reduce the dimensionality
        if conv_pooling[len(conv_filters)] > 0:
            model.add(
                MaxPooling2D(
                    pool_size=(conv_pooling[len(conv_filters)],
                               conv_pooling[len(conv_filters)]),
                    padding=padding))

        # flatten to make data digestible for dense layers
        model.add(Flatten())
        if batch_norm:
            model.add(BatchNormalization())

        # go through all passed dense layers
        for dense in dense_nodes:
            # add a new dense layer
            model.add(
                Dense(
                    dense,
                    activation=dense_activation,
                    **dense_regularize_params))

            # add batch normalization
            if batch_norm:
                model.add(BatchNormalization())

            # add dropout
            if dropout:
                model.add(Dropout(dropout))

        # prepare optimizer
        if optimizer == 'rmsprop':
            optimizer = RMSprop(lr=learning_rate if learning_rate else 0.001)
        elif optimizer == 'adam':
            optimizer = Adam(lr=learning_rate if learning_rate else 0.001)
        elif optimizer == 'sgd':
            optimizer = SGD(lr=learning_rate if learning_rate else 0.01)

        # final dense layer for numerical prediction
        model.add(Dense(1))

        # compile the model
        model.compile(
            loss=loss,
            optimizer=optimizer,
            metrics=['mean_squared_error', 'mean_absolute_error'])

        # print an overview about the model
        print(model.summary())
        print('\n')

        # if early stopping patience is set, configure the callback
        callbacks = []
        if patience > 0:
            callbacks.append(
                EarlyStopping(monitor='val_loss', patience=patience))
        if tensorboard:
            callbacks.append(
                TensorBoard(log_dir=current_path / f'00_LOGS/{time.time()!s}'))
        if lr_plateau:
            callbacks.append(
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=lr_plateau[0],
                    patience=lr_plateau[1],
                    min_lr=lr_plateau[2]))

        # always save the best model in a checkpoint file
        if self.cache_path is not None:
            best_path = str(self.cache_path / f'{self.cache_name}_best.h5')
            latest_path = str(self.cache_path / f'{self.cache_name}_latest.h5')
            callbacks.append(
                ModelCheckpoint(
                    filepath=best_path, verbose=1, save_best_only=True))
            callbacks.append(ModelCheckpoint(filepath=latest_path, verbose=1))

        # log results to CSV
        if self.log_path is not None:
            log_path = str(self.log_path / f'{self.log_name}.csv')
            callbacks.append(CSVLogger(log_path))

        # fit the model to the training data
        if validation_data:
            # always use the data given as validation_data for validation purposes
            history = model.fit(
                X_train,
                y_train,
                epochs=epochs,
                validation_data=validation_data,
                batch_size=batch_size,
                verbose=verbose,
                callbacks=callbacks)
        else:
            # use a percentage of the years as validation data (and shuffle)
            history = model.fit(
                X_train,
                y_train,
                epochs=epochs,
                validation_split=validation_split,
                batch_size=batch_size,
                verbose=verbose,
                callbacks=callbacks)

        # save the model and its history to cache
        model.save(self.cache_path / f'{self.cache_name}.h5')
        with open(self.cache_path / f'{self.cache_name}.history',
                  'wb') as file:
            pickle.dump(history.history, file)

        self.model = model
        self.history = history.history
