import os
import pathlib
import time
import pickle
import re
import hashlib
import numpy as np
import pandas as pd

from sklearn.preprocessing import normalize

from keras.models import Model, Sequential, load_model
from keras.layers import BatchNormalization, Dropout, LSTM, Dense, ConvLSTM2D, Flatten, MaxPooling2D, MaxPooling3D
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import Adam, RMSprop, SGD
from keras import backend as K
from keras.backend.tensorflow_backend import set_session
from keras.callbacks import Callback, EarlyStopping, TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger
from keras.regularizers import l1, l2, l1_l2

from ModelHelpers import ModelHelpers

# limit gpu resource usage of tensorflow
# see https://github.com/keras-team/keras/issues/1538
import tensorflow as tf
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

class ModelERAv1:
    """ Version 1 of ERA-based LSTM model """

    def __init__(self):
        self.cache_path = None
        self.log_path = None
        self.model = None
        self.history = None

    @staticmethod
    def train_test_split(datasets,
                         prediction_ts,
                         onset_ts,
                         years=range(1979, 2018),
                         years_train=range(1979, 2012),
                         years_dev=range(2012, 2015),
                         years_test=range(2015, 2018)):
        """
        Prepare data to be in a digestible format for the model

        :datasets: List of datasets to use as features
        :outcomes: Outcomes as generated by the base model

        :return:
        """

        def unstack_year(df):
            """ Unstack a single year and return an unstacked sequence of grids """

            return np.array(
                [df.iloc[:, i].unstack().values for i in range(df.shape[1])])

        def unstack_all(dataframes):
            """ Unstack all years and return the resulting dict """

            result = {}

            for year in years:
                result[year] = np.stack(
                    [unstack_year(df[year]) for df in dataframes], axis=-1)

            return result

        def reshape_years(arr):
            return np.array(
                list(
                    map(lambda year: year.reshape((year.shape[0], year.shape[1], year.shape[2], 2)),
                        arr)))

        def stack_outcomes(outcomes, years):
            return [outcomes[year] for year in years]

        def normalize_channels(arr):
            # normalize each channel seperately
            # axes 1 and 2 should be fixed (lat and lon)
            # axes 0 and 2 should be variable (for each channel seperately over all images)
            # see: https://stackoverflow.com/questions/42460217/how-to-normalize-a-4d-numpy-array
            arr_min = arr.min(axis=(1, 2), keepdims=True)
            arr_max = arr.max(axis=(1, 2), keepdims=True)

            return (arr - arr_min) / (arr_max - arr_min)

        # generate outcomes
        outcomes = ModelHelpers.generate_outcomes(
            prediction_ts, onset_ts, years, numerical=True)

        # unstack the entire dataset
        # => bring into matrix form with lat/lon on axes
        unstacked = unstack_all(datasets)

        print(unstacked[2017][0][0])
        print(f'> unstacked: {unstacked[2017].shape!s}')

        # generate training data
        X_train = reshape_years([unstacked[year] for year in years_train])
        X_train = normalize_channels(X_train)
        y_train = stack_outcomes(outcomes, years_train)
        print(X_train[0][0][0])
        print(f'> X_train: {X_train.shape!s}')

        # generate test data
        X_test = reshape_years([unstacked[year] for year in years_test])
        X_test = normalize_channels(X_test)
        y_test = stack_outcomes(outcomes, years_test)
        print(X_test[0][0][0])
        print(f'> X_test: {X_test.shape!s}')

        if years_dev:
            X_dev = reshape_years([unstacked[year] for year in years_dev])
            X_dev = normalize_channels(X_dev)
            y_dev = stack_outcomes(outcomes, years_dev)
            print(X_dev.shape)
            print(f'> X_dev: {X_dev.shape!s}')

            return X_train, y_train, X_test, y_test, X_dev, y_dev, unstacked

        return X_train, y_train, X_test, y_test, unstacked

    def build(self,
              X_train,
              y_train,
              dropout=0.6,
              dropout_recurrent=0.3,
              dropout_conv=0.4,
              epochs=50,
              optimizer='rmsprop',
              learning_rate=0.01,
              batch_size=1,
              loss='mean_squared_error',
              batch_norm=True,
              conv_activation='tanh',
              conv_filters=[16, 8, 4],
              conv_kernels=[7, 5, 3],
              conv_pooling=[2, 2, 2, 0],
              recurrent_activation='hard_sigmoid',
              padding='same',
              dense_nodes=[1024, 512, 256],
              dense_activation='relu',
              patience=0,
              validation_split=0.1,
              validation_data=None,
              verbose=1,
              invalidate=False,
              tensorboard=False,
              numerical=True,
              dense_kernel_regularizer=('L2', 0.02),
              conv_kernel_regularizer=('L2', 0.02),
              conv_recurrent_regularizer=('L2', 0.02),
              lr_plateau=(0.1, 5, 0.0001),
              cache_id=None,
              evaluate=None,
              version="E1"):
        """
        Build a stateless LSTM model

        :X_train: Training features
        :y_train: Training outcomes
        :dropout: How much dropout to use after dense layers
        :dropout_recurrent: How much recurrent dropout to use in ConvLSTM2D
        :dropout_conv: How much dropout to use after convolutional layers
        :epochs: How many epochs to train for
        :optimizer: The optimizer to use
        :learning_rate: The learning rate to use with the optimizer
        :batch_size: The batch size of training
        :loss: The loss function to use
        :batch_norm: Whether to use batch normalization
        :conv_activation: The activation to use in the convolutional layers
        :conv_filters: The number of filters for all convolutional layers as a list
        :conv_kernels: The dimensions of the kernels for all convolutional layers as a list
        :conv_pooling: Dimensions of the max pooling layers => one after each conv layer, final one before flatten
        :recurrent_activation: The activation for the LSTM recurrent part of ConvLSTM2D
        :padding: Whether to apply padding or not
        :numerical: Whether to train classes or numerical output
        :dense_nodes: The number of dense nodes for all dense layers as a list
        :dense_activation: The activation to use in all dense nodes except the final one
        :verbose: The level of logging to be used
        :invalidate: Whether the cache should be invalidated
        :patience: How much patience to use for early stopping
        :tensorboard: Whether tensorboard should be used
        :validation_split: How much of the data to keep for validation
        :conv_kernel_regularizer: Regularizer function applied to the kernel weights matrix of ConvLSTM2D layers.
        :conv_recurrent_regularizer: Regularizer function applied to the recurrent_kernel weights matrix of ConvLSTM2D layers.
        :dense_kernel_regularizer: Regularizer function applied to the kernel weights matrix of Dense layers.
        :lr_plateau: Whether the learning rate should be dynamically decreased when learning stagnates
        :cache_id: The id of the respective experiment.

        :return: The fitted model
        :return: The history of training the model
        """

        current_path = pathlib.Path(__file__).resolve().parent.parent.parent

        # define a path for the model to be cached to
        # => use md5 or similar for cache path..
        # define a path for the model to be cached to
        if cache_id is not None:
            # use the given cache id as a reference (array index)
            self.cache_path = current_path / '00_CACHE'
            self.cache_name = f'lstm_{version}_{cache_id}'
            self.log_path = current_path / '03_EVALUATION/histories'
            self.log_name = f'lstm_{version}_{cache_id}'

            # try to read the model version to evaluate from cache
            # could either be latest or best
            if evaluate is not None:
                model = load_model(self.cache_path /
                                   (self.cache_name + f'_{evaluate}.h5'))
                self.model = model
                return

        # try to read the model and its history from cache
        """ if not invalidate and os.path.isfile(self.cache_path + '.h5') and os.path.isfile(self.cache_path + '.history'):
            model = load_model(self.cache_path + '.h5')

            self.model = model
            return """

        # calculate the wanted input shape
        input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3],
                       X_train.shape[4])

        print(f'> input_shape: {input_shape!s}')

        # prepare regularizers for later use in the model
        regularizers = {'L2': l2, 'L1': l1, 'L1_L2': l1_l2}
        conv_regularize_params = dict()
        dense_regularize_params = dict()
        if (conv_kernel_regularizer is not None):
            reg = regularizers[conv_kernel_regularizer[0]]
            conv_regularize_params['kernel_regularizer'] = reg(
                conv_kernel_regularizer[1])
        if (conv_recurrent_regularizer is not None):
            reg = regularizers[conv_recurrent_regularizer[0]]
            conv_regularize_params['recurrent_regularizer'] = reg(
                conv_recurrent_regularizer[1])
        if (dense_kernel_regularizer is not None):
            reg = regularizers[dense_kernel_regularizer[0]]
            dense_regularize_params['kernel_regularizer'] = reg(
                dense_kernel_regularizer[1])

        # start building a sequential model
        model = Sequential()

        # go through all convolutional lstm layers
        for (index, filters) in enumerate(conv_filters):
            last_layer = index == len(conv_filters) - 1

            # add a ConvLSTM2D layer
            if index == 0:
                model.add(
                    ConvLSTM2D(
                        filters=filters,
                        kernel_size=(conv_kernels[index], conv_kernels[index]),
                        activation=conv_activation,
                        recurrent_activation=recurrent_activation,
                        padding=padding,
                        input_shape=input_shape,
                        return_sequences=not last_layer,
                        **conv_regularize_params))
            else:
                model.add(
                    ConvLSTM2D(
                        filters=filters,
                        kernel_size=(conv_kernels[index], conv_kernels[index]),
                        activation=conv_activation,
                        recurrent_activation=recurrent_activation,
                        padding=padding,
                        return_sequences=not last_layer,
                        **conv_regularize_params))

            # add a pooling layer if configured
            if conv_pooling[index] > 0:
                if last_layer:
                    model.add(
                        MaxPooling2D(
                            pool_size=(conv_pooling[index],
                                       conv_pooling[index])))
                else:
                    model.add(
                        MaxPooling3D(
                            pool_size=(1, conv_pooling[index],
                                       conv_pooling[index])))

            # add batch normalization
            if batch_norm:
                model.add(BatchNormalization())

            # add dropout
            if dropout:
                model.add(Dropout(dropout))

        # add max pooling before flattening to reduce the dimensionality
        if conv_pooling[len(conv_filters)] > 0:
            model.add(
                MaxPooling2D(
                    pool_size=(conv_pooling[len(conv_filters)],
                               conv_pooling[len(conv_filters)]),
                    padding=padding))

        # flatten to make data digestible for dense layers
        model.add(Flatten())
        if batch_norm:
            model.add(BatchNormalization())

        # go through all passed dense layers
        for dense in dense_nodes:
            # add a new dense layer
            model.add(
                Dense(
                    dense,
                    activation=dense_activation,
                    **dense_regularize_params))

            # add batch normalization
            if batch_norm:
                model.add(BatchNormalization())

            # add dropout
            if dropout:
                model.add(Dropout(dropout))

        # prepare optimizer
        if optimizer == 'rmsprop':
            optimizer = RMSprop(lr=learning_rate if learning_rate else 0.001)
        elif optimizer == 'adam':
            optimizer = Adam(lr=learning_rate if learning_rate else 0.001)
        elif optimizer == 'sgd':
            optimizer = SGD(lr=learning_rate if learning_rate else 0.01)

        # final dense layer for numerical prediction
        model.add(Dense(1))

        # compile the model
        model.compile(
            loss=loss,
            optimizer=optimizer,
            metrics=['mean_squared_error', 'mean_absolute_error'])

        # print an overview about the model
        print(model.summary())
        print('\n')

        # if early stopping patience is set, configure the callback
        callbacks = []
        if patience > 0:
            callbacks.append(
                EarlyStopping(monitor='val_loss', patience=patience))
        if tensorboard:
            callbacks.append(
                TensorBoard(log_dir=current_path / (
                    f'00_LOGS/{time.time()!s}')))
        if lr_plateau:
            callbacks.append(
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=lr_plateau[0],
                    patience=lr_plateau[1],
                    min_lr=lr_plateau[2]))

        # always save the best model in a checkpoint file
        if self.cache_path is not None:
            best_path = str(self.cache_path / f'{self.cache_name}_best.h5')
            latest_path = str(self.cache_path / f'{self.cache_name}_latest.h5')
            callbacks.append(
                ModelCheckpoint(
                    filepath=best_path, verbose=1, save_best_only=True))
            callbacks.append(ModelCheckpoint(filepath=latest_path, verbose=1))

        # log results to CSV
        if self.log_path is not None:
            log_path = str(self.log_path / f'{self.log_name}.csv')
            callbacks.append(CSVLogger(log_path))

        # fit the model to the training data
        if validation_data:
            # always use the data given as validation_data for validation purposes
            history = model.fit(
                X_train,
                y_train,
                epochs=epochs,
                validation_data=validation_data,
                batch_size=batch_size,
                verbose=verbose,
                callbacks=callbacks)
        else:
            # use a percentage of the years as validation data (and shuffle)
            history = model.fit(
                X_train,
                y_train,
                epochs=epochs,
                validation_split=validation_split,
                batch_size=batch_size,
                verbose=verbose,
                callbacks=callbacks)

        self.model = model
        self.history = history
